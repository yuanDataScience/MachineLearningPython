{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Using NLTK and Classification Models\n",
    "\n",
    "In this project, natural language processing techniques, including regular expression and several tools from NLTK package were used to clean and process the text data. A pipeline including text processing, numeric feature extraction and model optimization using GridSearchCV for Natural Language Processing was established. The pipeline was applied to [UCI SMS Spam Collection data set](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection), and [Yelp Reviews dataset from kaggle](https://www.kaggle.com/c/yelp-recsys-2013) datasets.\n",
    "\n",
    "Three classification models, multinomial naive bayer, logistic regression and support vector machine were used to predict the classes of the messages or reviews. Support vector machine showed better performance in both precision and recall values for both **SMS Spam** and **Yelp review** datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages and Data\n",
    "Among the loaded packages, PorterStemmer was loaded for word steaming and stopwords for identifying words that are commonly appear in documents and therefore, do not provide unique information for differentiating documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then, load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = pd.read_csv('smsspamcollection/SMSSpamCollection', sep='\\t',names=[\"label\", \"message\"])\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains two columns: 'label' column tells us if a message is ham or spam. This will be used as our target variable in classification models; 'message' column contains text content of the message. The purpose of this project is to predict whether or not new messages are spam messages based on the text content of messages using calssification models.\n",
    "\n",
    "\n",
    " Since natural language processing focues on the text processing. The graphic data exploration is usually not intensively used as in other machine learning applications. Here I just briefly explore the dataset uisng pandas **groupby** and **describe** functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " Since the natural language process primary focues on the text processing. The graphic data exploration is not intensive used as in other machine learning application. Here I just briefly explore the SMS Spam dataset uisng pandas **groupby** and **describe** functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4825   4516                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data summary shows that there are 4825 useful messages and 747 spam messages included in the dataset. In addition, the most frequently appeared useful message is \"Sorry, I'll call later\", while the most frequently appeared spam message is much longer, and contains key words such as \"customer\", \"service\" and \"representative\" that are more likely to occur in business conversations. In the next step, regular expression and some NLTK tools will be used to extract features from the text content of messages to differentiate the two message categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation and stop words removal\n",
    "This project used the bag-of-words model to convert text words in messages to numerical feature vectors. Actually, there are some useful tools in sklearn that can extract and convert text words to feature vectors. In feature vectors, the value of each element represents the frequency of the corresponding word that appears in the document, and each unique word has a unique position or index in the vector. The word frequency can also be \"normalized\" by considering the frequency of the words in the entire document pool (**Inverse Document Frequency**, idf adjustment). The idf \"normalized\" feature frequency may (or may not) help us to compare and differentiate documents better, depending on the problems. For example, even if some words appear in all the documents, but if their frequencies in some documents are much higher than in the others, and we can find certain patterns between their frequencies and the classes of documents, then integrating these words will still provide useful information to differentiate the documents. \n",
    "\n",
    "Special characters, such as punctuations, were removed using Python's built-in **string** library. Emoticons such as ':)' were extracted using regular expressions, and kept in the message, since these emoticons do contain meaningful information.\n",
    "\n",
    "Some common words (**stop words**), such as \"and\", \"is\", \"a\", etc. appear in almost every document and therefore may not provide very useful information to differentiate documents (this also depends on the problems). These stop words can be removed using the stop word list provided by NLTK package.\n",
    "\n",
    "Below is the text_process() function that was implemented to remove punctuation characters except for emoticons from messages. I extracted the stop word list from NLTK package and stored the list in a variable called 'stop', which can be used to remove stop words from messages.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_process(inputstring):\n",
    "    \"\"\"\n",
    "    input\n",
    "      inputString: a string text, with words separated by space\n",
    "    output\n",
    "      a list of strings that removed all punctuation and stop words\n",
    "    \n",
    "    \"\"\"\n",
    "    # remove punctuation characters\n",
    "    emotions=re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|p)',inputstring)\n",
    "    nopunc = ''.join([char for char in inputstring if char not in string.punctuation])+ ' '.join(emotions).replace('-','')\n",
    "        \n",
    "    return nopunc\n",
    "\n",
    "stop= stopwords.words('english')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test the text_process() function using a test input string of **\"This :) is :+ :( a test :-)@!\"**. This string contains emoticons and punctuations. We can see that the text_process function extracted the useful fragments from the input string text, and removed punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This  is   a test :) :( :)'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_process(\"This :) is :+ :( a test :-)@!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I remove all the punctuation characters from the message column using text_process function and apply function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "messages['message']=messages['message'].apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point crazy Available only in ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point crazy Available only in ...     111\n",
       "1   ham                            Ok lar Joking wif u oni      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham        U dun say so early hor U c already then say      49\n",
       "4   ham  Nah I dont think he goes to usf he lives aroun...      61"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Stemming\n",
    "Word stemming is the process of transforming a word into its root form. This allows us to map related words to the same stem. This can effectively reduce the size of the vocabulary vectors. Here I used the Porter stemmer algorithm implemented in NLTK. I implemented two functions to tokenize the input string: one with stemming, and the other just splitted the input string to a word list without stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    porter=PorterStemmer()\n",
    "    \n",
    "    return[porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "The pre-processed text messages are then converted to numeric feature vectors by TfidfVectorizer object in sklearn. In the next cell, I created two IfidfVectorizer objects. One used idf adjustment, one only used the raw word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "TFIDF_transformer=TfidfVectorizer(stop_words=stop,tokenizer=tokenizer).fit(messages['message'])\n",
    "TFIDF_transformer_NoIDF=TfidfVectorizer(stop_words=stop,tokenizer=tokenizer,use_idf=False,norm=None,smooth_idf=False).fit(messages['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the effects of these transformation, let's randomly take one text message from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U dun say so early hor U c already then say\n"
     ]
    }
   ],
   "source": [
    "message4 = messages['message'][3]\n",
    "print(message4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and see its vector representation after the transformation using and without using the idf adjustment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Frequency W/O adjustment: \n",
      "  (0, 1145)\t1.0\n",
      "  (0, 1930)\t1.0\n",
      "  (0, 3044)\t1.0\n",
      "  (0, 3065)\t1.0\n",
      "  (0, 4282)\t1.0\n",
      "  (0, 7328)\t2.0\n",
      "  (0, 8767)\t2.0\n",
      "\n",
      "\n",
      "TFIDF Frequency With adjustment: \n",
      "  (0, 8767)\t0.319491587697\n",
      "  (0, 7328)\t0.559700094048\n",
      "  (0, 4282)\t0.464527625007\n",
      "  (0, 3065)\t0.335574365269\n",
      "  (0, 3044)\t0.309125465389\n",
      "  (0, 1930)\t0.287037034059\n",
      "  (0, 1145)\t0.279850047024\n"
     ]
    }
   ],
   "source": [
    "TFIDF_message4_NoIDF=TFIDF_transformer_NoIDF.transform([message4])\n",
    "TFIDF_message4=TFIDF_transformer.transform([message4])\n",
    "\n",
    "print \"TFIDF Frequency W/O adjustment: \"\n",
    "print TFIDF_message4_NoIDF\n",
    "print \"\\n\"\n",
    "print \"TFIDF Frequency With adjustment: \"\n",
    "print TFIDF_message4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the printed list, we see that there are seven unique words in message number 4. It seems that in the results without adjustment, frequencies of some words are exactly twice of the others. Let's check some of the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already\n",
      "c\n",
      "say\n",
      "u\n"
     ]
    }
   ],
   "source": [
    "print(TFIDF_transformer.get_feature_names()[1145])\n",
    "print(TFIDF_transformer.get_feature_names()[1930])\n",
    "print(TFIDF_transformer.get_feature_names()[7328])\n",
    "print(TFIDF_transformer.get_feature_names()[8767])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparing the content of message 4, we observed that 'U' and 'say' appeared twice and other words only appeared once, which is consistent to the numbers obtained in the 'TFIDF Frequency W/O adjustment' treatment. To summarize, frequencies in 'TFIDF Frequency W/O adjustment' are the raw word frequencies in the message, while frequencies in 'TFIDF with adjustment' were idf adjusted.\n",
    "\n",
    "Next, let's check the effects of word stemming by setting the 'tokenizer' value of the TfidfVectorizer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TFIDF_transformer=TfidfVectorizer(stop_words=stop,tokenizer=tokenizer).fit(messages['message'])\n",
    "TFIDF_transformer_porter=TfidfVectorizer(stop_words=stop,tokenizer=tokenizer_porter).fit(messages['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Frequency W/O porter: \n",
      "  (0, 8902)\t0.400223752528\n",
      "  (0, 8461)\t0.354773054283\n",
      "  (0, 8442)\t0.266683165396\n",
      "  (0, 5780)\t0.405100532009\n",
      "  (0, 5134)\t0.449291680455\n",
      "  (0, 3852)\t0.356888309363\n",
      "  (0, 2948)\t0.228484387562\n",
      "  (0, 1309)\t0.311918709413\n",
      "\n",
      "\n",
      "TFIDF Frequency With porter stemming: \n",
      "  (0, 7628)\t0.423556459568\n",
      "  (0, 7251)\t0.37545602397\n",
      "  (0, 7235)\t0.269177568398\n",
      "  (0, 5023)\t0.428717551178\n",
      "  (0, 4479)\t0.336580477749\n",
      "  (0, 3374)\t0.377694596636\n",
      "  (0, 2612)\t0.241804834549\n",
      "  (0, 1248)\t0.330103306957\n"
     ]
    }
   ],
   "source": [
    "TFIDF_message4=TFIDF_transformer.transform([messages.loc[4,'message']])\n",
    "TFIDF_message4_porter=TFIDF_transformer_porter.transform([messages.loc[4,'message']])\n",
    "\n",
    "print \"TFIDF Frequency W/O porter: \"\n",
    "print TFIDF_message4\n",
    "print \"\\n\"\n",
    "print \"TFIDF Frequency With porter stemming: \"\n",
    "print TFIDF_message4_porter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see some of the words in the vocabulary lists of the two TfidfVectorizer objects with and without idf adjustment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usf\n",
      "though\n",
      "think\n",
      "nah\n",
      "lives\n"
     ]
    }
   ],
   "source": [
    "print(TFIDF_transformer.get_feature_names()[8902])\n",
    "print(TFIDF_transformer.get_feature_names()[8461])\n",
    "print(TFIDF_transformer.get_feature_names()[8442])\n",
    "print(TFIDF_transformer.get_feature_names()[5780])\n",
    "print(TFIDF_transformer.get_feature_names()[5134])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usf\n",
      "though\n",
      "think\n",
      "nah\n",
      "live\n"
     ]
    }
   ],
   "source": [
    "print(TFIDF_transformer_porter.get_feature_names()[7628])\n",
    "print(TFIDF_transformer_porter.get_feature_names()[7251])\n",
    "print(TFIDF_transformer_porter.get_feature_names()[7235])\n",
    "print(TFIDF_transformer_porter.get_feature_names()[5023])\n",
    "print(TFIDF_transformer_porter.get_feature_names()[4479])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the words in the two vocabulary lists, we can see the effects of word stemming, for example, 'lives' was converted to 'live'. Now, let's compare the sizes of the vocabulary lists of the two transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8333)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF_message4_porter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9708)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF_message4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation using porter stemming has a smaller vocabulary size than transformation without stemming. This is reasonable, since some words that are considered as different words before may have the same stems, and therefore, are considered as the same words after stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was splitted into training and test datasets. Three classification models were used to predict the message classes, including the multinomial naive bayer, logistic regression and support vector machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Feature_X=messages['message']\n",
    "target_y=messages['label'].apply(lambda x: 1 if x=='spam' else 0)\n",
    "X_train,X_test,y_train,y_test=train_test_split(Feature_X,target_y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1029       Lol you forgot it eh  Yes Ill bring it in babe\n",
       "5407    Yup he msg me is tat yijue Then i tot its my g...\n",
       "3252                            I‘ll leave around four ok\n",
       "1588    Dont search love let love find U Thats why its...\n",
       "4921       G says you never answer your texts confirmdeny\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3900L,)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1029    0\n",
       "5407    0\n",
       "3252    0\n",
       "1588    0\n",
       "4921    0\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I loaded the naive bayer, logistic regression and support vector machine models, GridSearchCV, Pipeline, classification_report and accuracy_score modules for model optimization and evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayer Classifier\n",
    "In this part of the project, I used multinomial naive bayer classifier with GridSearchCV to optimize the word pre-process procedures. The grid_param contains two dictionaries. One used idf adjustment as the default settings, and the other used raw word counts by setting use_idf=False, norm=None and smooth_idf=False. In addition, in each dictionary, the GridSearchCV compared and evaluated the model performance with and without stop word removal and word stemming procedures, and selected models based on the performance evaluated by the default 3-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_param=[{'vect__stop_words':[stop,None],\n",
    "             'vect__tokenizer':[tokenizer,tokenizer_porter]},\n",
    "            {'vect__stop_words':[stop,None],\n",
    "             'vect__tokenizer':[tokenizer,tokenizer_porter],\n",
    "             'vect__use_idf':[False],\n",
    "             'vect__norm':[None],\n",
    "             'vect__smooth_idf':[False]\n",
    "            \n",
    "            }\n",
    "           ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,...rue,\n",
       "        vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'vect__tokenizer': [<function tokenizer at 0x00000000131D1048>, <function tokenizer_porter at 0x000000001083CD68>], 'vect__stop_words': [[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'..., u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn'], None]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_tfidf=Pipeline([('vect',TfidfVectorizer(preprocessor=None,analyzer='word')),('clf',MultinomialNB())])\n",
    "gs=GridSearchCV(nb_tfidf,param_grid=grid_param)\n",
    "gs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multinomial naive bayer model was evaluated using the classification report. The model performance is pretty good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99      1445\n",
      "          1       0.95      0.90      0.93       227\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(y_test, gs.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Logistic regression model was applied to the dataset using the same GridSearchCV parameters and pipeline as the multinomial naive bayer model used in the last section. Performance of the best model was evaluated by classification report. The performance is comparable to the multinomial naive bayer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,...rue,\n",
       "        vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'vect__tokenizer': [<function tokenizer at 0x00000000131D1048>, <function tokenizer_porter at 0x000000001083CD68>], 'vect__stop_words': [[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'..., u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn'], None]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_tfidf=Pipeline([('vect',TfidfVectorizer(preprocessor=None,analyzer='word')),('clf',LogisticRegressionCV())])\n",
    "gs_lr=GridSearchCV(nb_tfidf,param_grid=grid_param)\n",
    "gs_lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99      1445\n",
      "          1       0.95      0.90      0.93       227\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(y_test, gs_lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "Finally, I evaluated the performance of SVM. Since both naive bayer and logistic regression showed very good classification performance, a linear kernel should be enough. Therefore, I used linear kernel and optimized C parameter. Results showed that SVM had better performance than both naive bayer and logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_param_svc=[{\n",
    "                'vect__stop_words':[stop,None],\n",
    "                'vect__tokenizer':[tokenizer,tokenizer_porter],\n",
    "                'clf__C' : [0.01,0.1,1,10.0]                \n",
    "                },\n",
    "            {'vect__stop_words':[stop,None],\n",
    "             'vect__tokenizer':[tokenizer,tokenizer_porter],\n",
    "             'vect__use_idf':[False],\n",
    "             'vect__norm':[None],\n",
    "             'vect__smooth_idf':[False],\n",
    "             'clf__C':[0.01,0.1,1,10.0] \n",
    "            }\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'vect__tokenizer': [<function tokenizer at 0x00000000131D1048>, <function tokenizer_porter at 0x000000001083CD68>], 'clf__C': [0.01, 0.1, 1, 10.0], 'vect__stop_words': [[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourse...tokenizer_porter at 0x000000001083CD68>], 'vect__use_idf': [False], 'clf__C': [0.01, 0.1, 1, 10.0]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_tfidf=Pipeline([('vect',TfidfVectorizer(preprocessor=None,analyzer='word')),('clf',SVC(kernel=\"linear\"))])\n",
    "gs_svc=GridSearchCV(svc_tfidf,param_grid=grid_param_svc)\n",
    "gs_svc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      1445\n",
      "          1       0.99      0.92      0.95       227\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(y_test, gs_svc.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Pipeline to Yelp Dataset\n",
    "Next, I applied the Natural Language Processing pipeline established in this project to[Yelp Reviews dataset from kaggle](https://www.kaggle.com/c/yelp-recsys-2013).  \n",
    "\n",
    "In this project, I only selected observations having 1 and 5 stars, and thus, converted the problem to a binary classification problem. The classification models then predicted and classified the reviews either as 1 or 5 stars. In addition, only the text content ('text' column) was used in model training and prediction. All the other columns in the dataset were dropped. The same text_process() procedure used in the **SMS Spam** dataset was applied in this dataset. For model training and evaluation, I used GridSearchCV with the same param_grid and pipelines as in the **SMS Spam** dataset. The same three models, multinomial naive bayer, logistic regression and support vector machine with linear kernel were used and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp=pd.read_csv(\"yelp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_df=yelp.loc[(yelp['stars']==1)|(yelp['stars']==5),:]\n",
    "X_yelp=yelp_df['text'].apply(text_process)\n",
    "y_yelp=yelp_df['stars']\n",
    "X_train_yelp,X_test_yelp,y_train_yelp,y_test_yelp=train_test_split(X_yelp,y_yelp,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_param=[{'vect__stop_words':[stop,None],\n",
    "             'vect__tokenizer':[tokenizer,tokenizer_porter]},\n",
    "            {'vect__stop_words':[stop,None],\n",
    "             'vect__tokenizer':[tokenizer,tokenizer_porter],\n",
    "             'vect__use_idf':[False],\n",
    "             'vect__norm':[None],\n",
    "             'vect__smooth_idf':[False]\n",
    "            \n",
    "            }\n",
    "           ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,...rue,\n",
       "        vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'vect__tokenizer': [<function tokenizer at 0x00000000131D1048>, <function tokenizer_porter at 0x000000001083CD68>], 'vect__stop_words': [[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'..., u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn'], None]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_tfidf=Pipeline([('vect',TfidfVectorizer(preprocessor=None,analyzer='word')),('clf',MultinomialNB())])\n",
    "gs_yelp=GridSearchCV(nb_tfidf,param_grid=grid_param)\n",
    "gs_yelp.fit(X_train_yelp,y_train_yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.88      0.67      0.76       224\n",
      "          5       0.93      0.98      0.95      1002\n",
      "\n",
      "avg / total       0.92      0.92      0.92      1226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(y_test_yelp,gs_yelp.predict(X_test_yelp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,...rue,\n",
       "        vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'vect__tokenizer': [<function tokenizer at 0x00000000131D1048>, <function tokenizer_porter at 0x000000001083CD68>], 'vect__stop_words': [[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'..., u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn'], None]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_tfidf=Pipeline([('vect',TfidfVectorizer(preprocessor=None,analyzer='word')),('clf',LogisticRegressionCV())])\n",
    "gs_lr_yelp=GridSearchCV(nb_tfidf,param_grid=grid_param)\n",
    "gs_lr_yelp.fit(X_train_yelp,y_train_yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.88      0.67      0.76       224\n",
      "          5       0.93      0.98      0.95      1002\n",
      "\n",
      "avg / total       0.92      0.92      0.92      1226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(y_test_yelp,gs_lr_yelp.predict(X_test_yelp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine with Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_param_svc=[{\n",
    "                'vect__stop_words':[stop,None],\n",
    "                'vect__tokenizer':[tokenizer,tokenizer_porter],\n",
    "                'clf__C' : [0.01,0.1,1,10.0]                \n",
    "                },\n",
    "            {'vect__stop_words':[stop,None],\n",
    "             'vect__tokenizer':[tokenizer,tokenizer_porter],\n",
    "             'vect__use_idf':[False],\n",
    "             'vect__norm':[None],\n",
    "             'vect__smooth_idf':[False],\n",
    "             'clf__C':[0.01,0.1,1,10.0]             \n",
    "            }\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'vect__tokenizer': [<function tokenizer at 0x00000000131D1048>, <function tokenizer_porter at 0x000000001083CD68>], 'clf__C': [0.01, 0.1, 1, 10.0], 'vect__stop_words': [[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourse...tokenizer_porter at 0x000000001083CD68>], 'vect__use_idf': [False], 'clf__C': [0.01, 0.1, 1, 10.0]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_tfidf=Pipeline([('vect',TfidfVectorizer(preprocessor=None,analyzer='word')),('clf',SVC(kernel=\"linear\"))])\n",
    "gs_svc_yelp=GridSearchCV(svc_tfidf,param_grid=grid_param_svc)\n",
    "gs_svc_yelp.fit(X_train_yelp,y_train_yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.92      0.74      0.82       224\n",
      "          5       0.94      0.99      0.96      1002\n",
      "\n",
      "avg / total       0.94      0.94      0.94      1226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(y_test_yelp,gs_svc_yelp.predict(X_test_yelp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "A pipeline including text processing, numeric feature extraction and model optimization using GridSearchCV for Natural Language Processing was established. The pipeline was applied to [UCI SMS Spam Collection data set](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection), and [Yelp Reviews dataset from kaggle](https://www.kaggle.com/c/yelp-recsys-2013) datasets. Three classification models, including multinomial naive bayer, logistic regression and support vector machine were used to predict the classes of the messages or reviews. Support vector machine showed better performance in both precision and recall values for both **SMS Spam** and **Yelp review** datasets. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
