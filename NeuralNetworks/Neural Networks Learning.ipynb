{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Learning for Digital Image Recognition\n",
    "This project implemented a three layer neural networks for digital image recognition. The networks consists of an input layer, one hidden layer and one output layer. The dataset contains 5000 digit images. Each corresponds to a 20 pixel by 20 pixel grayscale image of the digit (0-9). Each pixel is represented by a floating point number indicating the grayscale intensity at that location. The 20 by 20 grid of pixels have been converted to a 400-dimentional vector. The labels of the images are stored in another txt file, in which digit 0 is label as 10. For all the other digits from 1-9, labels and the digit images are consistent. This digit image dataset was splited into training and test datasets for the training and evaluation of the neural networks built in this project. Prediction accuray on the test dataset was used to evaluate the model. An accuracy of 0.939 was obtained on test dataset, with an optimum l2 lambda value of 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import optimize\n",
    "from scipy.special import expit\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Explore Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the digital image and label data. Since the digit image of 0 is labeled as 10 in the target_y, we convert the number 10 to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_X=pd.read_csv(\"ex4data1X.txt\",header=None).values\n",
    "target_y=pd.read_csv(\"ex4dataY.txt\",header=None)\n",
    "\n",
    "target_y=target_y[0]\n",
    "\n",
    "target_y[target_y==10]=0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will check the distribution of the label counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    500\n",
       "3    500\n",
       "6    500\n",
       "2    500\n",
       "9    500\n",
       "5    500\n",
       "1    500\n",
       "8    500\n",
       "4    500\n",
       "0    500\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the 9 digit numbers are evenly distributed in the total 5000 images, with each number having 500 images. Now we can check some of the digit images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we convert target_y from pandas Series to numpy array for model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_y=target_y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check some of the digit images stored in image_X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efe82060668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax=plt.subplots(nrows=2,ncols=5,sharex=True,sharey=True)\n",
    "ax=ax.flatten()\n",
    "for i in range(10):\n",
    "    img=image_X[target_y==i,:][0].reshape(20,20).T\n",
    "    #img=ndimage.rotate(img,-90)\n",
    "    ax[i].imshow(img,cmap=\"Greys\",interpolation=\"nearest\")\n",
    "    \n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It woruld be interesing to see the different styles of the same digit number. Let' check the different styles for digit number 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAG3RJREFUeJzt3X+Q1WX1wPGzuj8QXX4IKxYoBkqKaCHBlBrZENmoYQNC\naGglgz+gkn5RhvgjpFKIRvtBSogyoGahRlQ4liATZNY0MmkiaYaKiquArMDugu73j2ae7zlnd++9\nK3fvPffu+/XXc+YA3n32s3v8nOfzeZ6KlpYWAQAgmkOK/QEAAGgLBQoAEBIFCgAQEgUKABASBQoA\nEFJlpmR9fQOP+OWgrq62Ipc/x3zmJtf5FGFOc8F85hfzmX/tzSl3UACAkChQAICQKFAAgJAoUACA\nkChQAICQKFAAgJAoUACAkChQAICQKFAAgJAoUACAkChQAICQKFAAgJAoUACAkChQAICQKFAAgJAo\nUACAkChQAICQKFAAgJAoUACAkChQAICQKFAAgJAoUACAkChQAICQKFAAgJAoUACAkChQAICQKlpa\nWtpNNjc3t59EUl1dXZHLn9u/fz/zmYOqqqqc5lOEazRXuV6jBw4cYD5zUFlZmdN8NjU1MZ85qKmp\naXM+uYMCAIREgQIAhESBAgCERIECAIREgQIAhESBAgCERIECAIREgQIAhESBAgCERIECAIRUWewP\nkE1FRc673mSVaVsn/E+m+Wb+cuPn8JBDDmlz3JYDBw6kMfPdto78TmAOW89XZWXuv/bfeecdE7/9\n9tt5+Uy54g4KABASBQoAEBIFCgAQUsg1KN2nz9YD1fmqqiqT871XHXfV3vShhx5qYj+f+/fvb/fv\n+vnVuup8irReV9LrSCIif/3rX9P4tttuM7na2loTX3vttWnct29fk/M/C+VE/2xmW6fT12i2Oemq\n16z+Od+zZ4/JzZ8/38SbN29OYz/3l1xyiYnHjh2bxv73a2fMJ3dQAICQKFAAgJBCnKjrbytffPHF\nNP75z39ucg899JCJd+zYkcYXXXSRyXXv3t3EQ4cOTeNx48aZ3MHcrpbSibovvfSSiRcuXGjiO++8\nM427detmcv57ce6556axf3T1YG73S+FEXX3N+hbK7NmzTbx9+/Y0vuyyy0zOt/wmTpyYxhdccIHJ\nHcwjvtFO1PU/b7pVp3+mRUT+/Oc/m/inP/1pGv/97383uWOOOcbE06dPT+PPf/7zJnfYYYeZuCPX\nbLQTdf18vvzyy2k8b948k1u5cqWJR40alcZvvvmmyelrV0Rk7dq1ady/f3+TO5jrkxN1AQAlhQIF\nAAiJAgUACKkgj5ln2vpFROSVV14xsV5L8j3Qr371qyYeOXJkGn/3u981uT/84Q8mnjFjRhr7Nahy\n4h8lf/rpp9PYr2v4+dWPlT755JMm59dP9OPTgwYNMrlCb4nS2fw1u2/fvjS+6qqrTK6urs7Ed9xx\nRxr7ddEHHnjAxEcffXQa53Obr2LLtsb78MMPp/F1111ncn6tY8SIEWm8detWk/vPf/5j4quvvjqN\nhw0bZnKjR482cSlds/563Lt3r4mnTp2axgMHDjS5NWvWmFhfr/fdd5/J7dy508R9+vRJ40I8ps8d\nFAAgJAoUACAkChQAIKROW4PKtHWJ3lpDROTiiy82se5tPvjggyZ36qmnmlg/76/HIq23itHvQWTb\n8qeU+Pl96623THzppZem8YABA0xu/fr1Jj7yyCPT2H+fPvCBD5j43//+dxr7NahSl2mbLBH7vph/\nd+RnP/uZifW606ZNm0zOv5Ny5ZVXdvizlgI/f5neHfvYxz5mct///vdNrH92e/bsmfHPVldXp/Hh\nhx/egU8cm5/PpUuXmliv669YscLkjjrqKBOvW7cujW+//faM/65+d8xv6dUZuIMCAIREgQIAhESB\nAgCEVJA1qMcff9zk/J55fq3o17/+dRofe+yxJue319c90qeeesrkbr75ZhPrNZRyPrrAf2319fVp\nPHnyZJPz7+y88cYbabxhwwaT+/CHP2ziE088MY3L7egC3+P374QtXrw4jVevXm1yfo83vSb4hS98\nweROO+00Ex933HFpXM7XqL9eGhoa0vi1114zOf2OlIh9r8+/6+j3j/zBD36QxsOHDze5Ul539vR7\neT7esmWLyel1ORH7XpSfv8GDB5u40Nckd1AAgJAoUACAkPLW4vMtkcbGxjSeOXOmyfnHHP32GvpR\naN1yEml9GuS9996bxv5x9WnTppk400m95cSfiqtbJn/6059Mbtu2bSbWx5nox9NFWm/L07t37zQu\nt/n01/OSJUtMPGHChDTO1ob+xS9+kcb+MXPfhtaPQhfiMd5C8S09v+XTrFmz0thvdeSv2aampjT2\nj/iff/75Jv7sZz+bxoU4AbZQ/Gf325CtWrUqjadMmWJy/fr1M7FumY4fP97k/O9qWnwAAAgFCgAQ\nFAUKABBSpz1mrtdBdI9TROTyyy83sV8HueGGG9L40UcfNTm/JqX/O/p4Z5HWfe5y6ulnUlNTY2Ld\nh3/mmWdMzh/5rtdAfD/fP5Lu17pKmV+f0OscIiJ/+9vfTKy3j/HXlX5NQsQe+XDLLbeYnN/Wp5we\nfdb8monfnkuvd/q15FdffdXE+lH9559/3uT07w4RkSOOOCKNy2lu/VpQjx49TKzXoP74xz+anN7a\nSMT+ThgzZozJFfvIF+6gAAAhUaAAACFRoAAAIXXaGpTe7uXLX/6yyd10000mXr58uYknTZqUxv4Y\n90WLFplY95WHDBlicuX2bk57/Nfp195uu+22NPZHZOvtikTsu2P+aI5yXsPzvXa99Y5I6yNM9BrU\nCy+8YHL6vScRkVGjRqWxPvJFpPVaTFe5Zv2alJ5/fxTO/fffb+KNGzem8bJly0zulFNOMXE5rTtl\n4q+bXr16pfGFF15ocvr9RRF7xMYHP/jBTvh07x53UACAkChQAICQ8tbi87fs+jb9mmuuMTnf8quq\nqjKxfjRU78QtIvK1r33NxHpndL9LbylvZXIwMn0v/KPhvm2ndy+urLSXRzm3S3yLxLdB/OPL+qTn\noUOHmtzAgQNNrHfU9qe6lvOcdoRudfrTD+bOnWti/Zj5eeed16mfqxz436FXXHGFib/xjW+ksW/5\nF7vlzB0UACAkChQAICQKFAAgpE57zFyvg/i1DL9lTqZHTv3f3bNnj4n1Y5F+DaqcH4vuCN1HfvbZ\nZ03uhBNOMPEZZ5yRxl11Da8tftuncePGpfHs2bNNzq+LnH766Wlc7J5+FP7x+u3bt6ex3wpNP6Yv\nIrJgwYI01uvVIl13TS/TUSL+1GF/Deptpvzv22L/DuUOCgAQEgUKABASBQoAEFKnrUFpfi0jW59Y\n96ebm5sz/tmu2nPOxPeR//KXv6TxV77yFZPz7/ecfPLJacwa1P/zayZ6Tv1WXf5oDv0eWlddg/Lz\nt2/fPhPrdSe/zdQ999xj4p49e6ZxsddIovDzq999uvHGG03OH/Fy/PHHp3G036fcQQEAQqJAAQBC\nKkiLr6P0I5Ovv/66yfmWn7+17Yr8HOzcudPEeofyM8880+QmT55sYj33XbnF5x/b9dfd4sWL09if\n5NyvXz8TR2ubFEKmx55FRO666y4Tr1+/Po317toiIieddJKJaetln9+77747jf3p0HprI/9vRWtB\n89sdABASBQoAEBIFCgAQUsg1KN0H1Y9Aioj07dvXxP6RVC1bn7ZU+a/Lr3Fcf/317f5df/RJVz3R\nNRs/Lxs2bDDxc889l8bz5883uXK5zjpKX5d+/tasWWNif4SGjv22Ul11PjPx87t582YT62ty4cKF\nJjds2DATR17T4w4KABASBQoAEBIFCgAQUsg1KK179+4mnjlzpomXLl2axmPHjjU5v35VLr1svwb1\nyCOPmHjFihUmvu+++9J4yJAhJuePgMf/+Gvle9/7noknTZqUxn369DG5rjqnel3kscceMzn9Lp6I\nyMSJE02sj3zQW0OJlM/P7cHwP/P+Gps1a5aJBw8enMZnn322yZXSfHIHBQAIiQIFAAiJAgUACKki\nUz+yubm56M1K/7z/7t27Tax7r7t27TK5ZcuWmbiqqiqN89mHra6ursj+p0T279//rv+jeh78Ow/+\nCA2/v97UqVPTuBT2hauqqsppPkU67xr1PX+9jiciMmbMmDT27+ZFfJcs12v0wIEDOc+n/9nctGlT\nGs+ZM8fkxo8fb+IpU6aYWB8RE3H+vMrKypzms6mpKS/Xp78e/f56S5YsMfHo0aPT2L/35EVYk6qp\nqWlzPrmDAgCERIECAIQUvsXn+bbC3r170/ill14yuUGDBplYP75aai0+fYu/Y8cOk9uzZ4+J+/fv\n3+7fLQURWnyebg+L2FZpKbSkOqPF568rfYqrz/k2qBehzdQRhW7xeX5+M12fpdDWp8UHACgpFCgA\nQEgUKABASBnXoOrrG0qrMVwkdXW1OfWjmc/c5DqfIsxpLpjP/GI+86+9OeUOCgAQEgUKABASBQoA\nEBIFCgAQEgUKABASBQoAEBIFCgAQEgUKABASBQoAEBIFCgAQEgUKABASBQoAEBIFCgAQEgUKABAS\nBQoAEBIFCgAQEgUKABASBQoAEBIFCgAQEgUKABASBQoAEBIFCgAQEgUKABASBQoAEBIFCgAQEgUK\nABBSRUtLS7E/AwAArXAHBQAIiQIFAAiJAgUACIkCBQAIiQIFAAiJAgUACIkCBQAIiQIFAAiJAgUA\nCIkCBQAIqTJTsr6+gX2QclBXV1uRy59jPnOT63yKMKe5YD7zi/nMv/bmlDsoAEBIFCgAQEgUKABA\nSBQoAEBIFCgAQEgUKABASBQoAEBIFCgAQEgUKABASBQoAEBIFCgAQEgUKABASBQoAEBIFCgAQEgU\nKABASBQoAEBIFCgAQEgUKABASBQoAEBIFCgAQEgUKABASBQoAEBIFCgAQEgUKABASBQoAEBIFCgA\nQEgVLS0t7Sabm5vbTyKprq6uyOXPMZ+5yXU+RUSampqY0xzU1NRwjeZRrtdoY2Mj85mDbt26tTmf\n3EEBAEKiQAEAQqJAAQBCqiz2BwAA5FdFRUWb42z8MwmZnlEoBO6gAAAhUaAAACHR4itDhxxi/7+j\nsvL/v83+lv3AgQMmLvYtPYCO8208/XPc0NBgcvv27Wv37/bq1cvkqqurTfzOO+8c1OfsKO6gAAAh\nUaAAACFRoAAAIZXcGtTBPDJZrvya0549e0z86KOPpnHfvn1NbsSIEe3+W11l/t6NTNch89a5mPvW\nc7B//34T/+QnP0njZcuWmdyrr75q4kMPPTSNJ0+ebHLXXnutiXv06JHGhZhr7qAAACFRoAAAIVGg\nAAAhhVyD0j3Rt99+2+R8r1X3Qf1ajP53RGzftpR71b7/7N9rmDlzponvvPPONH7ve99rcuvXrzfx\ncccdl8Z+7suZn1N/LWW7DjX93llH6f9utvnvyHpsKfFz7+l39/x7Of5nvlzXVP0c3X///SbWa0df\n+tKXTG7atGkm1tfZddddZ3Jbt2418fDhw9M4089AvnAHBQAIiQIFAAgpxIm6/rb8jTfeSOMbb7zR\n5Pwjk7qdcswxx5jcmDFjTHzOOeek8ZlnnvnuPmwbCn2irp+vLVu2mPj00083sb5t14+Jioj8/ve/\nN7FuB3br1s3kCrXNSaFO1M3UTtu0aZOJ7733XhPreWpubja5+fPnm/jSSy9N42ytxG3btqXxvHnz\nTO7kk0828eWXX25ifV34n+toJ+r6edA/x2+99ZbJPfDAAyb+8Y9/nMb/+te/TG7ixIkmnjt3bhof\nffTRJncwLb9Cn6jr52vnzp0mPuOMM0xcW1ubxqtXrzY53+bXP9dr1qwxuWHDhplYb3105JFHZvvY\nOeNEXQBASaFAAQBCokABAEIqyGPmvn/q1zLWrl1r4quvvjqNn376aZObNWuWifV6y1133WVyP/zh\nD028ffv2NPY921J+ZNfPZ58+fUx81llnpfGOHTtM7qmnnjLx66+/nsZ+Ta/UZXo8f/ny5Sb3zW9+\n08QDBgww8YIFC9JYr5mKtF5D0d+fmpoak9PzLSJy2WWXpfEjjzxicnrtRaT1+lXkx6j9Z21qajLx\nqlWr0tivM/tr9NOf/nQaX3DBBSY3Z84cE+vvm9+2p5T4+XvllVdM7Nekzj///DT225v5I3b0z8XI\nkSNNzj+i3tjYmMa//OUvM37GfOAOCgAQEgUKABASBQoAEFJB1qB8b/yee+4x8de//nUT6/eXbr31\nVpMbNWqUiXVP1Pfohw4damK9fuXfJSr0Ucb55OfXb0Gi+/3++Gffu9brJ6W8LtcW3yPX7zLdfffd\nJqePKBERGThwoIn1OyD+2vHvVOl3R3bt2mVyN998s4kffvjhNJ4xY4bJ6fepRDIf811s2bbj8j/z\ner3Nv/81duzYdv/tpUuXmpzeqktEZNKkSWnsv/+l9DPvP6u/Hv1R7XoLM/9O34knnmhifd0sWrTI\n5Pw7aIMGDWrz74l0zvXIHRQAICQKFAAgJAoUACCkTluD0ms8vgfq3zG55JJLTKx70N27dzc538PX\ne2397ne/Mzm/rfwJJ5yQxpH69R3lP3u/fv1MfPzxx5v4mmuuSWO/v57vG+s1EN+rjrzmkQu/BvGP\nf/wjjf115vcgy7Qvof93/Tqf/u/od/xERDZu3Ghivd7i3+kppTUUv8br1/T8nm/r1q1L4/e9730m\n56+7zZs3p7E/HsLvxTdkyJA0LrXrVfOfXe+1J9L6utLrqxMmTDA5/36j/refffZZk+vfv7+J9fuj\nhXgPjzsoAEBIFCgAQEid1uLT7YcNGzaYnN/mZPr06SbW7RS/1dG3v/1tE+ttUI444giT0y09kczH\nEZQS39rp3bu3ie+44w4T6/aKbnmItD5uQ28z88UvftHk/PyW2hz6x79nz56dxv7oCv+1+yMK9Nfu\nW1D//Oc/2439VlO+haKPl/FbVpXSCce+/ePb/GeffbaJM53kvHv3bhPrbab8tlL+1RK9tVQhToAt\nFP874NRTTzWxPmHXX4/+e3Hsscem8UknnWRy/u/qx86znWacD9xBAQBCokABAEKiQAEAQuq0NahM\njy4efvjhJl65cqWJ9Vbyv/3tb03uiiuuMPHUqVPT2K9PfeITnzBxpiO+S5nvBfujIT73uc+lse8T\n+7UVfbS5fwTarxuUGj9P+nHmJUuWmJxfx3viiSdMvHfv3jT2/X8/T/qxc/+axPXXX2/i0047LY3L\n+Rr91a9+1e6f1WsiIiKPP/64ifWatn8k3T9SXS5z6Nd7H3zwQRP7V0L0+uXo0aNN7qMf/aiJ9e/F\nqqoqk/PHwTz55JPtfia2OgIAdBkUKABASBQoAEBInbYGpfuaF198scnpreBFRL7zne+YWK+hzJ8/\n3+Q+85nPmFhvdeLf8Xn/+99v4nLpR2fjv07dG/ZrAe95z3tMfOGFF6axf0fqk5/8ZL4+Ygh6nvy6\nnT8e3L+7p/vr/hh333vX75L4Yyf08eUisbcv6gh/rLjfzuywww4zsb7W/FqGfyetvr4+jf17T/77\nWMrzqX+H6jVPkdZHFn384x83sZ6XbHOQ6feif09PH/PR3Nxscn79Kh+4gwIAhESBAgCEVJCtjvzO\n0GvXrjWxb3voE0j1yaUirR/3/dGPfpTGN910k8lVVtovr6u0+LxMj3v623L9aL4+gVikdZtLf59K\nbdsjL1sbRH+t2fj2y6pVq9J44cKFJtezZ08Tl8s1mu3Vh6uuusrEeqspf03qLalE7HWot4YSKf0d\n99vj22l+iye/RdG7PQ3b/z3/vdDXp/9M/vWhfOAOCgAQEgUKABASBQoAEFKnrUFpfut9f2yDjzX/\nuKo+KVJEZOTIkWmst/QRKZ/+c0d1pP/s50j3nP22PNu3bzexfuS0XNZO2pNpWxd/jfptkk455ZQ0\n9o+Vd5VrNNv1odcvtm3bZnKrV682sX61xJ8mXa7XoV8L8r8z9TqniN3eyG9v5tcHMz3Ofuutt5pY\nn2bcGcdreNxBAQBCokABAEKiQAEAQirIGpSX7Z0T3d/3PdEVK1aY+Morr0xj/05JV+nvZ3v3Q/fl\nfd/Yrw9q/j0H/75aV6bn/LnnnjO5F1980cS6j1+u7z11lL9m9btN3/rWt0yurq7OxOecc04al/PP\neKbttPS6pkjrd0Bra2vTeObMmSbn3y1tbGxM49tvv93k/BrULbfcksb+Wu6MbaW4gwIAhESBAgCE\nVJQWXza67bRmzRqT69Wrl4knT56cxr591VXaJ75N99BDD5lYt0x0e0REZM6cOSbWrRffPinndko2\nviWl2yI33HCDyfnWh34VoqvOoZ8/3w7S22r5E3T9Y+b6EetS3q08G32t+G3bZsyYYeJnnnnGxLrl\n57eW+9SnPmXi3/zmN2m8ZcuWjP+diy66KNvHzivuoAAAIVGgAAAhUaAAACFVZOqJNzc3F6Rh7vur\nW7duTePzzjvP5HQ/X8RuK+O3nCmU6urqnPYW6qz59P19fYKriMh///vfNF60aJHJ+XU7/b14/vnn\nTW7dunUm7t27dxrncy0g1/kUEWlqairINern6YUXXkjjc8891+R8PG/evM77YDmqqakp6jXq5++x\nxx4z8fjx49N45cqVJveRj3zExBHWlnO9RhsbGztlPv26sz9+Q299pNeYRERee+01E+s10wkTJpjc\nuHHjTNyjR480zud6ardu3dqcT+6gAAAhUaAAACFRoAAAIRXlPSjfP/Vbw0yZMiWN/REP06dPN3FX\nfa9E83Pgt0EZPnx4Gusj3UVEFixYYOLly5en8dy5c03Ob5ESYS2gWPTX7q/ns846y8R6fS7T1lLl\nxH+dL7/8somnTZtm4okTJ6bxhz70IZMr53ed3i0/J/64dX30kF7fE2n9c6u/V927dzc5/7ul0N+L\nrvHTAgAoORQoAEBIFCgAQEhFWYPy7+088cQTJt64cWMaL1682ORGjBhh4q68DpIr/X6Y7zHr47N9\nXOz+cyT+az/qqKPS2K+ZLFu2zMT6mu3bt2/Gf7dc+DUofVS4SOt3FvXasj/enJ/x7DIdseOP6sgk\n2lxzBwUACIkCBQAIqShbHfkW3+7du028bdu2NB48eLDJVVdXmzjCY+bF3uqo3ETc6sjT192uXbtM\nzm/ro7eHKZZCb3Xkf8YbGhpM/Oabb5p4wIABaRzhZzqbYm91VG7Y6ggAUFIoUACAkChQAICQMq5B\n1dc30D/NQV1dbU79aOYzN7nOpwhzmgvmM7+Yz/xrb065gwIAhESBAgCERIECAIREgQIAhESBAgCE\nRIECAIREgQIAhESBAgCERIECAIREgQIAhESBAgCERIECAIREgQIAhESBAgCERIECAIREgQIAhESB\nAgCERIECAIREgQIAhESBAgCERIECAIREgQIAhESBAgCERIECAIREgQIAhESBAgCEVNHS0lLszwAA\nQCvcQQEAQqJAAQBCokABAEKiQAEAQqJAAQBCokABAEL6PyimaNA8zYPcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efe81c02080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax=plt.subplots(nrows=2,ncols=5,sharex=True,sharey=True)\n",
    "ax=ax.flatten()\n",
    "for i in range(10):\n",
    "    img=image_X[target_y==9,:][i].reshape(20,20).T\n",
    "    ax[i].imshow(img,cmap=\"Greys\",interpolation=\"nearest\")\n",
    "    \n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will implement a simple nerual networks using the feedforward and backpropagation algorithms. Below is a very concise explanation of functions implemented in the model. For the details of the derivation of the equations and algorithm implementation considerations, please refer to the [Neural Networks Learning]( http://htmlpreview.github.io/?https://github.com/yuanDataScience/Machine_Learning_R/blob/master/NeuralNetworksLearning/Neural_Networks_Learning.nb.html) project in my github.\n",
    "It should be noted that the purpose of this project is to help people to understand the feed forward and back propagation algorithms. Therefore, the implementation is very slow when training large nerual networks and large datasets. The model can not be used for big datasets such as the MNIST digital image datasets.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural networks model implemented the following functions:\n",
    "    1. __init__(): function that initializes an instance of a neural networks model, and defines the network structure and hyperparameter (lmd for the l2 regularization)\n",
    "    2. _costCompute() and _gradCompute(): functions that calculate the cost and gradient of weight parameters using feed forward and back propagation alogrithms, respectively\n",
    "    3. _randomInitializeWeights(): function that generates the initial networks paramters based on uniform distribution\n",
    "    4 _debugInitializeWeights(): This function is used for testing and debugging. Given the numbers of neurons in the neighboring two layers, the function generates simulated weight parameters connecting the two layers using sine function. By using this function, we can establish small size simulated neural networks containing only a few neurons in each layer with the corresponding weight parameters. Network weight gradients calculated by back propagation and numeric methods for this simulated neural networks can be compared to check if the back propagation algorithm is properly implemented. \n",
    "    5. _computeNumericGrad(): This function calculates the gradient using the numerical method by introducing a small disturbance to each weight parameter and calculates the diffence in cost function divided by the magnitude of the disturbance. It can only be used on small neural network. It is used in this project to check if the back propagation algorithm is properly implemented.\n",
    "    6. checkNNGradients(): This function creates a small, simulated neural networks using __debugInitializeWeights(), and compares the gradient of weight parameters obtained by numerical and back propagration algorithms.\n",
    "    7. fit(): trains the neural networks by optimizing the weight parameters using the training dataset\n",
    "    8. predict(): predicts the target variables using the trained neural networks model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "class ann(BaseEstimator,ClassifierMixin):\n",
    "    def __init__(self,lmd=1,input_layer_size=400,hidden_layer_size=25,number_labels=10):\n",
    "        self.lmd=float(lmd)\n",
    "        self.input_layer_size=input_layer_size\n",
    "        self.hidden_layer_size=hidden_layer_size\n",
    "        self.number_labels=number_labels\n",
    "        \n",
    "    def _costCompute(self,theta,X,y):\n",
    "        '''\n",
    "        This function calculates the cost function based on the feed forward algorithm\n",
    "        input\n",
    "           theta: vector including network parameters from input to hidden, and from hidden to output layer\n",
    "           X: feaature matrix, exclusive of the intercept\n",
    "           y: target variable vector\n",
    "           \n",
    "        output\n",
    "           The cost of the output layer calculated by feedforward algorithm\n",
    "        '''\n",
    "        # get the sizes of the parameters from input to hidden layer (t1_element)\n",
    "        t1_element=(self.input_layer_size+1)*self.hidden_layer_size \n",
    "        t2_start_idx=t1_element\n",
    "        num_obs=X.shape[0]\n",
    "        \n",
    "        t1=theta[:t1_element].reshape((self.hidden_layer_size,-1)) #hidden_layer_size * input_layer_size+1\n",
    "        t2=theta[t2_start_idx:].reshape((self.number_labels,-1)) #number_labels * hidden_layer_size+1\n",
    "        \n",
    "        intercept=np.ones((num_obs,1))\n",
    "        X=np.hstack((intercept,X))\n",
    "        h=expit(np.hstack((intercept,expit(X.dot(t1.T)))).dot(t2.T))\n",
    "        \n",
    "        onehot=np.zeros((num_obs,self.number_labels))\n",
    "        for idx, label in enumerate(y):\n",
    "            onehot[idx,label]=1.0\n",
    "            \n",
    "        lld=onehot*np.log(h)+(1-onehot)*np.log(1-h)    \n",
    "            \n",
    "        return -1.0/num_obs*lld.sum() +0.5*(self.lmd/num_obs)*((t1[:,1:]*t1[:,1:]).sum()+(t2[:,1:]*t2[:,1:]).sum())\n",
    "    \n",
    "    def _gradCompute(self,theta,X,y):\n",
    "        '''\n",
    "        This function calculates the gradient of weight parameters, theta, using back propogation algorithm\n",
    "        input\n",
    "           theta: vector including network parameters from input to hidden, and from hidden to output layer\n",
    "           input_layer_size: the number of features in input matrix\n",
    "           hidden_layer_size: the number of nodes in the hidden layer\n",
    "           number_labels: the number of the labels, in this project is 10. This also equals the number of output layer nodes\n",
    "           X: feaature matrix, exclusive of the intercept\n",
    "           y: target variable vector\n",
    "           lmd: lambda hyper-parameter defining regularization\n",
    "        output\n",
    "           The gradient of network parameters as a vector\n",
    "        '''\n",
    "        t1_element=(self.input_layer_size+1)*self.hidden_layer_size\n",
    "        t2_start_idx=t1_element\n",
    "        \n",
    "        num_obs=X.shape[0]\n",
    "       \n",
    "        t1=theta[:t1_element].reshape((self.hidden_layer_size,-1)) # hidden_layer_size*input_layer_size+1\n",
    "        t2=theta[t2_start_idx:].reshape((self.number_labels,-1)) #number_labers*(hidden_layer_size+1)\n",
    "        onehot=np.zeros((num_obs,self.number_labels))\n",
    "        for idx, label in enumerate(y):\n",
    "            onehot[idx,label]=1.0\n",
    "            \n",
    "        intercept=np.ones((num_obs,1))\n",
    "        X=np.hstack((intercept,X))\n",
    "        \n",
    "        X_middle_pred=expit(X.dot(t1.T)) # num_obs*hidden_layer_size prediction\n",
    "        \n",
    "        output_layer=expit(np.hstack((intercept,X_middle_pred)).dot(t2.T)) #num_obs*number_label\n",
    "        delta_output_layer=output_layer-onehot #num_obs*number_label\n",
    "        \n",
    "        \n",
    "        error_transfer_middle=delta_output_layer.dot(t2[:,1:]) #num_obs*hidden_layer_size\n",
    "        delta_middle=error_transfer_middle*X_middle_pred*(1-X_middle_pred) #num_obs*hidden_layer_size\n",
    "        \n",
    "        grad_t2=(1.0/num_obs)*delta_output_layer.T.dot(np.hstack((intercept,X_middle_pred)))\n",
    "        grad_t2[:,1:]+=(self.lmd/num_obs)*t2[:,1:]\n",
    "        \n",
    "        grad_t1=(1.0/num_obs)*delta_middle.T.dot(X)\n",
    "        grad_t1[:,1:]+=(self.lmd/num_obs)*t1[:,1:]\n",
    "        \n",
    "        return np.concatenate((grad_t1.ravel(),grad_t2.ravel()))\n",
    "       \n",
    "    \n",
    "    \n",
    "    def _randInitializeWeights(self,L_in, L_out):\n",
    "        '''\n",
    "        This function initialize the weight parameters connecting two neighboring layers\n",
    "        , given the number of nodes in input and outpur layers. \n",
    "        '''\n",
    "        epsilon_init=0.12\n",
    "        w=(np.random.uniform(size=(L_in+1)*L_out)*epsilon_init*2-epsilon_init).reshape((L_out,L_in+1))\n",
    "        return(w)\n",
    "    \n",
    "    def _computeNumericGrad(self,theta,X, y):\n",
    "        '''\n",
    "        This function calculates the gradient of weight parameters, theta, using numeric difference in the cost function,\n",
    "        given very small change (1e-4) in each weight parameter. This function is used to check if _gradCompute() function\n",
    "        is correctly implemented.\n",
    "        input\n",
    "           theta: vector including network parameters from input to hidden, and from hidden to output layer\n",
    "           input_layer_size: the number of features in input matrix\n",
    "           hidden_layer_size: the number of nodes in the hidden layer\n",
    "           number_labels: the number of the labels, in this project is 10. This also equals the number of output layer nodes\n",
    "           X: feaature matrix, exclusive of the intercept\n",
    "           y: target variable vector\n",
    "           lmd: lambda hyper-parameter defining regularization\n",
    "        output\n",
    "           The gradient of network parameters as a vector\n",
    "        '''\n",
    "        para_size=len(theta)\n",
    "        e=1e-4\n",
    "        num_grad=np.zeros(para_size)\n",
    "        disturb=np.zeros(para_size)\n",
    "        for idx in range(para_size):\n",
    "            disturb[idx]=e\n",
    "            loss1=self._costCompute(theta-disturb,X,y)\n",
    "            loss2=self._costCompute(theta+disturb,X,y)\n",
    "            num_grad[idx]=(loss2-loss1)/(2*e)\n",
    "            disturb[idx]=0.0\n",
    "            \n",
    "        return num_grad\n",
    "    \n",
    "    def _debugInitializeWeights(self,fout,fin):\n",
    "        '''\n",
    "        This function is used for debugging. It is used in this project to check if the _gradCompute() function\n",
    "        is correctly implemented using the network weight parameters generated by this function.\n",
    "        \n",
    "        This function generates the simulated network weight parameters, given the numbers of nodes \n",
    "        in input and output layers of the neighboring layers using sine function. As a result, for\n",
    "        every neighboring layers in the network with a fixed numbers of input and output layer nodes,\n",
    "        the function will generate the corresponding simulated weight paramters connecting these two layers.\n",
    "        \n",
    "        input\n",
    "           fout: the number of nodes in the output layer of the two neighboring layers\n",
    "           fin: the number of nodes in the input layer of the two neighboring layers.\n",
    "        \n",
    "        '''\n",
    "        total_element=fout*(fin+1)\n",
    "        w=np.sin(np.arange(1,total_element+1)).reshape((fout,fin+1))\n",
    "        return w\n",
    "        \n",
    "    \n",
    "    def checkNNGradients(self,lmd=0):\n",
    "        '''\n",
    "        This function checks if the gradient of weight parameters are correctly calcualted by _gradCompute() function by\n",
    "        comparing the networks weight parameter gradient obtained using numeric method (_computeNumericGrad) and \n",
    "        back propogation algorithm (_gradCompute()) using a small simulated neural network and input data generated by \n",
    "        _debugInitializedWeights() function.\n",
    "        \n",
    "        input\n",
    "           lmd: lambda value for regularized logistic regression\n",
    "           \n",
    "        output:\n",
    "            prints normalized difference between the gradient calculated by numerica and back propogation algorithms.\n",
    "            returns the gradients calculated by the two algorithms as pandas dataframe.\n",
    "        \n",
    "        '''\n",
    "        self.lmd=float(lmd)\n",
    "        self.input_layer_size=3\n",
    "        self.hidden_layer_size=5\n",
    "        self.number_labels=3\n",
    "        m=5\n",
    "        \n",
    "        Theta1=self._debugInitializeWeights(self.hidden_layer_size,self.input_layer_size)\n",
    "        Theta2=self._debugInitializeWeights(self.number_labels,self.hidden_layer_size)\n",
    "        \n",
    "        feature_X=self._debugInitializeWeights(m,self.input_layer_size-1)\n",
    "        target_y=np.arange(m)%self.number_labels\n",
    "        \n",
    "        nnpara=np.concatenate((Theta1.ravel(),Theta2.ravel()))\n",
    "        numgrad=self._computeNumericGrad(nnpara,feature_X, target_y)\n",
    "        grad=self._gradCompute(nnpara,feature_X,target_y)\n",
    "        \n",
    "        diff=numgrad-grad\n",
    "        total=numgrad+grad\n",
    "        \n",
    "        norm_diff=np.sqrt(diff.dot(diff)/total.dot(total))\n",
    "        print(\"the normal of the difference is \",norm_diff, \"\\n\")\n",
    "        return pd.DataFrame({'numgrad':numgrad,'grad':grad})\n",
    "        \n",
    "       \n",
    "    def fit(self,X,y):\n",
    "        '''\n",
    "        This function trains the neural netwok by finding the optimum network weight parameters using the cost and gradient\n",
    "        functions, and the cg min algorithm implemented by scipy.optimize package\n",
    "        \n",
    "        input\n",
    "          X: feature matrix of the training dataset\n",
    "          y:target variable vector of the training dataset\n",
    "          \n",
    "        output\n",
    "           the optimum network weight parameters trained by traning data\n",
    "        '''\n",
    "        \n",
    "        theta1=self._randInitializeWeights(self.input_layer_size, self.hidden_layer_size)\n",
    "        theta2=self._randInitializeWeights(self.hidden_layer_size,self.number_labels)\n",
    "        theta=np.concatenate((theta1.ravel(),theta2.ravel()))\n",
    "        theta1_size=(self.input_layer_size+1)*self.hidden_layer_size\n",
    "                \n",
    "        rs=optimize.minimize(fun=self._costCompute, x0=theta, args=(X, y),method='CG',jac=self._gradCompute)\n",
    "        if rs.success:\n",
    "            self.theta1=rs.x[:theta1_size].reshape((self.hidden_layer_size,self.input_layer_size+1))\n",
    "            self.theta2=rs.x[theta1_size:].reshape(self.number_labels,self.hidden_layer_size+1)\n",
    "        return self\n",
    "    \n",
    "    def predict(self,new_X):\n",
    "        '''\n",
    "        This function predicts the new observations using the training neural network\n",
    "        \n",
    "        input\n",
    "           new_X: feature matrix for new predictios\n",
    "        \n",
    "        output\n",
    "           the vector of the target variables predicted by the neural network\n",
    "        '''\n",
    "        num_obs=new_X.shape[0]        \n",
    "        intercept=np.ones((num_obs,1))\n",
    "        new_X=np.hstack((intercept,new_X))\n",
    "        \n",
    "        h=expit(np.hstack((intercept,expit(new_X.dot(self.theta1.T)))).dot(self.theta2.T))\n",
    "        return np.argmax(h,axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check if the gradient calculation function is correctly implemented by the checkNNGradients() function. This function creates a small simulated network containing 3, 5 and 3 neurons in the input, hidden and output layers, respectively. It also generates a small training dataset containing 5 observations. The function compares the gradient calculated by numerical and back propagation algorithms based on the simulated network and training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the normal of the difference is  4.7829618344e-11 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grad</th>\n",
       "      <th>numgrad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.155567</td>\n",
       "      <td>0.155567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.552258</td>\n",
       "      <td>0.552258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.096990</td>\n",
       "      <td>0.096990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.447450</td>\n",
       "      <td>-0.447450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.068618</td>\n",
       "      <td>0.068618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.146390</td>\n",
       "      <td>-0.146390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.420729</td>\n",
       "      <td>0.420729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.601031</td>\n",
       "      <td>0.601031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.063594</td>\n",
       "      <td>-0.063594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.349509</td>\n",
       "      <td>-0.349509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.637875</td>\n",
       "      <td>-0.637875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.339782</td>\n",
       "      <td>-0.339782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.136126</td>\n",
       "      <td>-0.136126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.590879</td>\n",
       "      <td>0.590879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.384103</td>\n",
       "      <td>0.384103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.175816</td>\n",
       "      <td>-0.175816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.090363</td>\n",
       "      <td>-0.090363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.459059</td>\n",
       "      <td>-0.459059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.079808</td>\n",
       "      <td>0.079808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.545300</td>\n",
       "      <td>0.545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.216549</td>\n",
       "      <td>0.216549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.685713</td>\n",
       "      <td>0.685713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.196381</td>\n",
       "      <td>0.196381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.381906</td>\n",
       "      <td>-0.381906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.426827</td>\n",
       "      <td>-0.426827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.082795</td>\n",
       "      <td>-0.082795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.210130</td>\n",
       "      <td>0.210130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.773193</td>\n",
       "      <td>0.773193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.314249</td>\n",
       "      <td>0.314249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.249433</td>\n",
       "      <td>-0.249433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.430911</td>\n",
       "      <td>-0.430911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.286789</td>\n",
       "      <td>-0.286789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.395064</td>\n",
       "      <td>0.395064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.859114</td>\n",
       "      <td>0.859114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.497964</td>\n",
       "      <td>0.497964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.073752</td>\n",
       "      <td>0.073752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.369390</td>\n",
       "      <td>-0.369390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.340111</td>\n",
       "      <td>-0.340111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        grad   numgrad\n",
       "0   0.155567  0.155567\n",
       "1   0.552258  0.552258\n",
       "2   0.096990  0.096990\n",
       "3  -0.447450 -0.447450\n",
       "4   0.068618  0.068618\n",
       "5  -0.146390 -0.146390\n",
       "6   0.420729  0.420729\n",
       "7   0.601031  0.601031\n",
       "8  -0.063594 -0.063594\n",
       "9  -0.349509 -0.349509\n",
       "10 -0.637875 -0.637875\n",
       "11 -0.339782 -0.339782\n",
       "12 -0.136126 -0.136126\n",
       "13  0.590879  0.590879\n",
       "14  0.384103  0.384103\n",
       "15 -0.175816 -0.175816\n",
       "16 -0.090363 -0.090363\n",
       "17 -0.459059 -0.459059\n",
       "18  0.079808  0.079808\n",
       "19  0.545300  0.545300\n",
       "20  0.216549  0.216549\n",
       "21  0.685713  0.685713\n",
       "22  0.196381  0.196381\n",
       "23 -0.381906 -0.381906\n",
       "24 -0.426827 -0.426827\n",
       "25 -0.082795 -0.082795\n",
       "26  0.210130  0.210130\n",
       "27  0.773193  0.773193\n",
       "28  0.314249  0.314249\n",
       "29 -0.249433 -0.249433\n",
       "30 -0.430911 -0.430911\n",
       "31 -0.286789 -0.286789\n",
       "32  0.395064  0.395064\n",
       "33  0.859114  0.859114\n",
       "34  0.497964  0.497964\n",
       "35  0.073752  0.073752\n",
       "36 -0.369390 -0.369390\n",
       "37 -0.340111 -0.340111"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ann=ann()\n",
    "Ann.checkNNGradients(lmd=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results showed a very small normalized difference. In addition, comparing the columns of the gradients calculated by back propagation and numerical methods we can see the numbers are very close to each other. Therefore, we are confident that the back propagation algorithm was correctly implemented.\n",
    "Next, we will instantiate a neural network model object and train the model using the training dataset. The networks contains an input layer, which contains 400 nodes, corresponding to 400 features of each digit image in the training datase. In addition, the network contains a hidden layer with 25 nodes, and an output layer containing 10 nodes corresponding to the 10 unique digit numbers as the output. In this implementation, one-vs-all strategy was used by assigning the digit number with the highest output to the observation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ann1=ann()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We splitted the dataset to train and test datasets using train_test_split function in sklearn, trained the model using training dataset, and then used the prediction accuracy of test dataset to evaluate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(image_X,target_y,test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ann(hidden_layer_size=25, input_layer_size=400, lmd=1.0, number_labels=10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the test dataset accuracy is  0.931333333333\n",
      "the training dataset accuracy is  0.996\n"
     ]
    }
   ],
   "source": [
    "y_pred=ann1.predict(X_test)\n",
    "\n",
    "print(\"the test dataset accuracy is \", np.mean(y_pred==y_test))\n",
    "print(\"the training dataset accuracy is \",np.mean(ann1.predict(X_train)==y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It seems that the training dataset has a higher accuracy than test dataset. Therefore, the model might be over-fitted. we will check if we can optimize the lambda hyper-parameter by increasing the lmd value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best accuracy result:  0.939333333333\n"
     ]
    }
   ],
   "source": [
    "accuracy_result=[]\n",
    "for lmdpara in [1.5,2.0,2.5]:\n",
    "    ann_opt=ann(lmd=lmdpara)\n",
    "    ann_opt.fit(X_train,y_train)\n",
    "    accuracy_result.append(np.mean(ann_opt.predict(X_test)==y_test))\n",
    "    \n",
    "print(\"the best accuracy result: \",accuracy_result[np.argmax(accuracy_result)])     \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.93933333333333335, 0.93200000000000005, 0.92733333333333334]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test dataset accuracy inceased from 0.931 to 0.939 when lmd value increased from 1.0 to 1.5. We then checked the other lmd values in the range between 1.0 and 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best accuracy result:  0.936\n"
     ]
    }
   ],
   "source": [
    "accuracy_result1=[]\n",
    "for lmdpara in [1.1,1.2,1.3,1.4,1.6,1.7,1.8,1.9]:\n",
    "    ann_opt=ann(lmd=lmdpara)\n",
    "    ann_opt.fit(X_train,y_train)\n",
    "    accuracy_result1.append(np.mean(ann_opt.predict(X_test)==y_test))\n",
    "    \n",
    "print(\"the best accuracy result: \",accuracy_result1[np.argmax(accuracy_result1)])     \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results showed that the best result was obtained when lmd=1.5, corresponding to a test dataset accuracy of 0.939. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions: A neural networks model that implemented feed forward and back propagation algorithms was applied to the digit image recoginition. Network weight parameters were optimized using CG min function of Scipy package. In addition, the implementation included functions to check the implementation of back propagation algorithm by comparing the weight gradient vectors calculated by numerical method and back propagation algorithm using a small, simulated network structure. An acurracy of 93.39% based on test dataset was obtained at the optimimum l2 lambda value of 1.5  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
